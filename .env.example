# ============================================================================
# Arquivo de Configuração - SiCooperative Data Lake POC
# ============================================================================
# IMPORTANTE: Copie este arquivo para .env e ajuste os valores conforme necessário
# Comando: cp .env.example .env
# ============================================================================

# ----------------------------------------------------------------------------
# CONFIGURAÇÕES DO MYSQL
# ----------------------------------------------------------------------------

# Host do MySQL (use 'mysql' se estiver usando Docker Compose)
MYSQL_HOST=localhost

# Porta do MySQL
MYSQL_PORT=3306

# Nome do banco de dados
MYSQL_DATABASE=sicooperative_db

# Usuário do MySQL
MYSQL_USER=root

# Senha do MySQL (NUNCA commitar este arquivo com senha real!)
MYSQL_PASS=

# ----------------------------------------------------------------------------
# CONFIGURAÇÕES DO SPARK
# ----------------------------------------------------------------------------

# Nome da aplicação Spark
SPARK_APP_NAME=SiCooperative-ETL

# Master do Spark (local[*] usa todos os cores disponíveis)
SPARK_MASTER=local[*]

# Memória do driver
SPARK_DRIVER_MEMORY=2g

# Memória do executor
SPARK_EXECUTOR_MEMORY=2g

# Habilitar otimização adaptativa
SPARK_SQL_ADAPTIVE_ENABLED=true

# Habilitar coalesce de partições
SPARK_SQL_ADAPTIVE_COALESCE_PARTITIONS=true

# ----------------------------------------------------------------------------
# CONFIGURAÇÕES DE OUTPUT
# ----------------------------------------------------------------------------

# Diretório de saída para o CSV
OUTPUT_DIR=./output

# Nome do arquivo de saída
OUTPUT_FILENAME=movimento_flat.csv

# Modo de escrita (overwrite ou append)
OUTPUT_MODE=overwrite

# Incluir header no CSV
OUTPUT_HEADER=true

# Delimitador do CSV
OUTPUT_DELIMITER=,

# Encoding do arquivo
OUTPUT_ENCODING=UTF-8

# ----------------------------------------------------------------------------
# CONFIGURAÇÕES DE LOGGING
# ----------------------------------------------------------------------------

# Nível de log (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# Formatos de saída (csv,parquet ou ambos separados por vírgula)
OUTPUT_FORMAT=csv,parquet

# Configurações do Parquet (opções: none, snappy, gzip, lzo, brotli, lz4)
PARQUET_COMPRESSION=snappy

# ----------------------------------------------------------------------------
# CONFIGURAÇÕES DE LGPD
# ----------------------------------------------------------------------------
# Hash salt para o hash (NUNCA commitar este arquivo com hash salt real!)
HASH_SALT=seu_salt_secreto_aqui

# ----------------------------------------------------------------------------
# CONFIGURAÇÕES DE QUALIDADE DE DADOS
# ----------------------------------------------------------------------------

# Habilitar verificações de qualidade de dados
DATA_QUALITY_CHECKS_ENABLED=true

# Limite de NULL para campos críticos (1% = 0.01)
NULL_CHECK_THRESHOLD_CARTAO=0.01

# Limite para transações negativas (0% = 0.0)
NEGATIVE_TRANSACTIONS_THRESHOLD=0.0

# Tolerância para mudança de volume (50% = 0.5)
VOLUME_CHANGE_TOLERANCE=0.5

# Arquivo de histórico de qualidade
DATA_QUALITY_HISTORY_FILE=data_quality_history.json

# ----------------------------------------------------------------------------
# CONFIGURAÇÕES DE OBSERVABILIDADE
# ----------------------------------------------------------------------------

# Habilitar sistema de observabilidade e métricas
OBSERVABILITY_ENABLED=true

# Prometheus Pushgateway (opcional - deixe vazio para desabilitar)
# PROMETHEUS_GATEWAY_URL=http://localhost:9091

# Nome do job no Prometheus
PROMETHEUS_JOB_NAME=sicooperative-etl

# Habilitar logs detalhados de métricas
METRICS_DETAILED_LOGGING=false

# Arquivo de exportação de métricas (formato JSON)
METRICS_EXPORT_FILE=pipeline_metrics.json
# Desafio TÃ©cnico - Engenharia de Dados
## Data Lake POC - SiCooperative LTDA

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PySpark](https://img.shields.io/badge/PySpark-3.5-orange.svg)](https://spark.apache.org/)
[![MySQL](https://img.shields.io/badge/MySQL-8.0-blue.svg)](https://www.mysql.com/)
[![Docker](https://img.shields.io/badge/Docker-Compose-blue.svg)](https://www.docker.com/)

---

---

## ğŸ“‹ Ãndice

- [Contexto do Desafio](#-contexto-do-desafio)
- [Arquitetura da SoluÃ§Ã£o](#-arquitetura-da-soluÃ§Ã£o)
- [DecisÃµes TÃ©cnicas](#-decisÃµes-tÃ©cnicas)
- [Estrutura do Projeto](#-estrutura-do-projeto)
- [PrÃ©-requisitos](#-prÃ©-requisitos)
- [InstalaÃ§Ã£o e ExecuÃ§Ã£o](#-instalaÃ§Ã£o-e-execuÃ§Ã£o)
- [Testes](#-testes)
- [Observabilidade e MÃ©tricas](#-observabilidade-e-mÃ©tricas)
- [Melhorias Futuras](#-melhorias-futuras)
- [Dificuldades Encontradas](#-dificuldades-encontradas)
- [Modelo de Dados](#-modelo-de-dados)
- [RelatÃ³rio de ValidaÃ§Ã£o](#-relatÃ³rio-de-validaÃ§Ã£o)

---

## ğŸ¯ Contexto do Desafio

### Problema de NegÃ³cio

A **SiCooperative LTDA** Ã© uma empresa que oferece soluÃ§Ãµes financeiras justas, mas enfrentava desafios na tomada de decisÃµes devido Ã :

- **FragmentaÃ§Ã£o de dados**: InformaÃ§Ãµes dispersas em mÃºltiplos sistemas
- **Processos manuais**: Tempo perdido criando e correlacionando relatÃ³rios individuais
- **Necessidade de ML**: Nova equipe de Data Science precisa de dados consolidados para modelos preditivos

### SoluÃ§Ã£o Proposta

Criar uma **POC de Data Lake** para:

1. Centralizar dados de movimentaÃ§Ãµes de cartÃµes
2. Agregar informaÃ§Ãµes de cartÃµes, contas e associados
3. Gerar visÃ£o Ãºnica para anÃ¡lise e modelagem preditiva

### Objetivo do Pipeline

Extrair dados de 4 tabelas relacionais (MySQL) e gerar um arquivo CSV flat com visÃ£o consolidada de movimentaÃ§Ãµes, incluindo:

- Dados do associado (cliente)
- InformaÃ§Ãµes da conta
- Detalhes do cartÃ£o
- TransaÃ§Ãµes realizadas

---

## ğŸ—ï¸ Arquitetura da SoluÃ§Ã£o

### VisÃ£o Geral

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MySQL     â”‚â”€â”€â”€â”€â”€â–¶â”‚   PySpark    â”‚â”€â”€â”€â”€â”€â–¶â”‚  CSV File   â”‚
â”‚  (Source)   â”‚      â”‚ ETL Pipeline â”‚      â”‚  (Output)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Bronze              Silver                 Gold
```

### Arquitetura MedalhÃ£o (Simplificada)

Embora o desafio solicite apenas um CSV final, a soluÃ§Ã£o segue os princÃ­pios da **Arquitetura MedalhÃ£o**, amplamente adotada em Data Lakes modernos:

#### **ğŸ¥‰ Bronze Layer (Raw Data)**
- **RepresentaÃ§Ã£o**: Tabelas no MySQL
- **CaracterÃ­sticas**: Dados transacionais normalizados, exatamente como gerados pelo sistema operacional
- **Tabelas**: `movimento`, `cartao`, `conta`, `associado`

#### **ğŸ¥ˆ Silver Layer (Cleansed & Joined)**
- **RepresentaÃ§Ã£o**: TransformaÃ§Ãµes no Spark (em memÃ³ria)
- **OperaÃ§Ãµes**:
  - JOINs entre tabelas relacionais
  - RenomeaÃ§Ã£o de colunas para padrÃ£o consistente
  - ValidaÃ§Ã£o de integridade referencial
  - ConversÃ£o de tipos de dados

#### **ğŸ¥‡ Gold Layer (Curated & Aggregated)**
- **RepresentaÃ§Ã£o**: Arquivo CSV final
- **CaracterÃ­sticas**: Dados desnormalizados, prontos para consumo por analistas e cientistas de dados
- **Formato**: Flat file com todas as informaÃ§Ãµes agregadas

### Fluxo de Dados

```
MySQL: Associado â”€â”€â”
MySQL: Conta â”€â”€â”€â”€â”€â”€â”¼â”€â”€â–¶ Spark JOIN â”€â”€â–¶ TransformaÃ§Ãµes â”€â”€â–¶ CSV Output
MySQL: CartÃ£o â”€â”€â”€â”€â”€â”¤
MySQL: Movimento â”€â”€â”˜
```

**Detalhamento do Fluxo:**

1. **ExtraÃ§Ã£o (Extract)**
   - ConexÃ£o JDBC com MySQL
   - Leitura paralela das 4 tabelas
   - Carregamento em DataFrames Spark

2. **TransformaÃ§Ã£o (Transform)**
   - JOIN: `movimento` â† `cartao` â† `conta` â† `associado`
   - SeleÃ§Ã£o e renomeaÃ§Ã£o de colunas
   - ConversÃ£o de tipos (timestamps, decimais)

3. **Carga (Load)**
   - Escrita em CSV com header
   - DiretÃ³rio parametrizÃ¡vel via CLI
   - Encoding UTF-8

---

## ğŸ¨ DecisÃµes TÃ©cnicas

### 1. Stack TecnolÃ³gica

| Componente | Tecnologia | Justificativa |
|------------|------------|---------------|
| **Banco de Dados** | MySQL 8.0 | JÃ¡ instalado no ambiente, robusto para dados transacionais, excelente suporte JDBC |
| **Framework Big Data** | Apache Spark (PySpark) | Requisito do desafio, processamento distribuÃ­do, escalÃ¡vel para grandes volumes |
| **Linguagem** | Python 3.10+ | Ecossistema rico para dados, sintaxe clara, integraÃ§Ã£o nativa com Spark |
| **OrquestraÃ§Ã£o** | Docker Compose | Ambiente reproduzÃ­vel, isolamento de dependÃªncias, fÃ¡cil setup |
| **Testes** | pytest + chispa | Framework padrÃ£o Python, chispa especializado em DataFrames Spark |

### 2. Modelagem de Dados

#### **Schema Relacional (Bronze)**

Mantive a modelagem normalizada (3FN) no MySQL:

- **Vantagens**:
  - Integridade referencial garantida
  - ReduÃ§Ã£o de redundÃ¢ncia
  - Reflete sistema transacional real
  - Facilita manutenÃ§Ã£o e evoluÃ§Ã£o

- **Relacionamentos**:
  ```
  Associado (1) â”€â”€< (N) Conta
  Associado (1) â”€â”€< (N) CartÃ£o
  Conta (1) â”€â”€< (N) CartÃ£o
  CartÃ£o (1) â”€â”€< (N) Movimento
  ```

#### **Schema Flat (Gold)**

DesnormalizaÃ§Ã£o completa no CSV final:

- **Vantagens**:
  - Queries simples (sem JOINs)
  - Performance para leitura analÃ­tica
  - CompatÃ­vel com ferramentas de BI
  - Pronto para ingestÃ£o em modelos ML

- **Trade-off**: RedundÃ¢ncia de dados (aceitÃ¡vel para camada Gold)

### 3. Pipeline ETL

#### **EstratÃ©gia de JOIN**

```python
# Ordem otimizada de JOINs
movimento â†’ cartao â†’ conta â†’ associado
```

**Justificativa**:
- ComeÃ§a pela tabela de fatos (`movimento`) - maior volume
- JOINs sequenciais com dimensÃµes menores
- Spark otimiza automaticamente com broadcast para tabelas pequenas

#### **ConfiguraÃ§Ãµes Spark**

```python
spark = SparkSession.builder \
    .appName("SiCooperative-ETL") \
    .config("spark.sql.adaptive.enabled", "true")  # OtimizaÃ§Ã£o adaptativa
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
    .getOrCreate()
```

**Por quÃª?**
- **Adaptive Query Execution**: Ajusta plano de execuÃ§Ã£o em runtime
- **Coalesce Partitions**: Reduz partiÃ§Ãµes pequenas apÃ³s JOINs
- Melhora performance sem tuning manual

#### **Tratamento de Erros**

- **ValidaÃ§Ã£o de conexÃ£o**: Testa conectividade MySQL antes de iniciar
- **Logging estruturado**: Registra cada etapa do pipeline
- **ExceÃ§Ãµes customizadas**: Mensagens claras para troubleshooting

### 4. ContainerizaÃ§Ã£o

#### **Docker Compose Multi-Container**

```yaml
services:
  mysql:      # Banco de dados
  spark:      # Ambiente PySpark
```

**BenefÃ­cios**:
- **Reprodutibilidade**: Mesmo ambiente em qualquer mÃ¡quina
- **Isolamento**: DependÃªncias nÃ£o conflitam com sistema host
- **AutomaÃ§Ã£o**: Setup completo com um comando
- **Networking**: Containers se comunicam via rede interna

#### **Volumes Docker**

- `./sql:/docker-entrypoint-initdb.d`: Scripts SQL executados automaticamente
- `./output:/app/output`: PersistÃªncia do CSV gerado
- `./src:/app/src`: Hot-reload para desenvolvimento

### 5. Qualidade de CÃ³digo

#### **Testes UnitÃ¡rios**

```python
# Cobertura de testes
- TransformaÃ§Ãµes de dados
- LÃ³gica de JOIN
- ValidaÃ§Ã£o de schema
- Tratamento de erros
```

**Framework**: pytest + chispa
- `chispa.assert_df_equality()`: Compara DataFrames Spark
- Fixtures para dados de teste
- Mocks para conexÃµes externas

#### **Boas PrÃ¡ticas**

- **Type hints**: AnotaÃ§Ãµes de tipo em todas as funÃ§Ãµes
- **Docstrings**: DocumentaÃ§Ã£o inline no cÃ³digo
- **ConfiguraÃ§Ã£o externa**: `.env` para credenciais e parÃ¢metros
- **Logging**: Rastreabilidade de execuÃ§Ã£o

---

## ğŸ” Tratamento de Tipos de Dados

Para garantir a mÃ¡xima confiabilidade e consistÃªncia dos dados, foram implementadas as seguintes prÃ¡ticas de tipagem:

### 1. Valores MonetÃ¡rios (Decimal)
- **Formato**: `DECIMAL(20,2)` para todos os campos monetÃ¡rios
- **BenefÃ­cios**:
  - PrecisÃ£o exata em cÃ¡lculos financeiros
  - Evita erros de arredondamento em operaÃ§Ãµes matemÃ¡ticas
  - ConsistÃªncia na representaÃ§Ã£o de valores monetÃ¡rios
  - Compatibilidade com sistemas financeiros

### 2. Datas e HorÃ¡rios (ISO 8601)
- **Formato**: `YYYY-MM-DDTHH:MM:SSZ` (UTC)
- **Exemplo**: `2023-10-14T15:30:00Z`
- **BenefÃ­cios**:
  - PadrÃ£o internacional reconhecido
  - OrdenaÃ§Ã£o cronolÃ³gica correta em texto
  - Suporte a fuso horÃ¡rio (sempre UTC)
  - Evita ambiguidades (DD/MM/YYYY vs MM/DD/YYYY)
  - Compatibilidade com a maioria dos bancos de dados e ferramentas

### 3. Tipagem ExplÃ­cita
- **Campos numÃ©ricos**: ConversÃ£o explÃ­cita para tipos apropriados (INT, BIGINT, DECIMAL)
- **Campos de texto**: CodificaÃ§Ã£o UTF-8 para suporte a caracteres especiais
- **ValidaÃ§Ã£o de esquema**: VerificaÃ§Ã£o de tipos em tempo de execuÃ§Ã£o

### 4. PreservaÃ§Ã£o de Tipos no Output
- **CSV**: FormataÃ§Ã£o explÃ­cita de datas e decimais
- **Parquet**: ManutenÃ§Ã£o dos tipos nativos do Spark
- **Metadados**: Schema JSON incluÃ­do para referÃªncia

Esta abordagem garante que os dados mantenham sua integridade em todo o pipeline, desde a extraÃ§Ã£o atÃ© a anÃ¡lise final, reduzindo significativamente o risco de erros de interpretaÃ§Ã£o ou perda de precisÃ£o.

## ğŸ” ProteÃ§Ã£o de Dados Pessoais (PII)

O pipeline implementa estratÃ©gias rigorosas de proteÃ§Ã£o de dados pessoais identificÃ¡veis (PII) para garantir conformidade com leis de privacidade como LGPD e GDPR.

### EstratÃ©gia de ProteÃ§Ã£o

#### Dados SensÃ­veis Tratados

1. **NÃºmeros de CartÃ£o (PAN)**
   - **Mascaramento**: Mostra apenas primeiros 6 e Ãºltimos 4 dÃ­gitos
   - **Formato Final**: `123456******3456`
   - **Reversibilidade**: âŒ IrreversÃ­vel

2. **Emails de Associados**
   - **Mascaramento**: Oculta nome de usuÃ¡rio, mantÃ©m domÃ­nio
   - **Formato Final**: `joa****@email.com`
   - **Hash SHA-256**: Para anÃ¡lise estatÃ­stica sem revelar dados reais

#### CaracterÃ­sticas TÃ©cnicas

- **Algoritmo de Hash**: SHA-256 (irreversÃ­vel)
- **Salt ConfigurÃ¡vel**: SeguranÃ§a adicional contra ataques de rainbow table
- **ValidaÃ§Ãµes AutomÃ¡ticas**: Garante que dados estejam adequadamente protegidos
- **Auditoria Completa**: Logs detalhados de todas as validaÃ§Ãµes

#### Colunas no Output Final

| Coluna | Tipo | ProteÃ§Ã£o | Reversibilidade |
|--------|------|----------|-----------------|
| `numero_cartao_masked` | String | Mascarado | âŒ IrreversÃ­vel |
| `numero_cartao_hash` | String | Hash SHA-256 | âŒ IrreversÃ­vel |
| `email_masked` | String | Mascarado | âŒ IrreversÃ­vel |
| `email_hash` | String | Hash SHA-256 | âŒ IrreversÃ­vel |

### Conformidade RegulatÃ³ria

- **LGPD (Brasil)**: Atende requisitos de anonimizaÃ§Ã£o
- **GDPR (Europa)**: CompatÃ­vel com princÃ­pios de proteÃ§Ã£o de dados
- **PCI DSS**: NÃ£o armazena dados completos de cartÃ£o
- **SOX**: MantÃ©m auditoria sem comprometer privacidade

### Trade-offs Avaliados

**Hash IrreversÃ­vel (Implementado)**:
- âœ… MÃ¡xima proteÃ§Ã£o de privacidade
- âœ… Conformidade regulatÃ³ria
- âœ… Simplicidade de implementaÃ§Ã£o

**Token ReversÃ­vel (Rejeitado)**:
- âŒ Complexidade adicional
- âŒ Risco de vazamento de chaves
- âŒ NÃ£o atende anonimizaÃ§Ã£o LGPD

> ğŸ“‹ **Detalhes tÃ©cnicos**: Para implementaÃ§Ã£o tÃ©cnica completa, consulte [`src/data_quality.md`](src/data_quality.md)

## ğŸ“ Estrutura do Projeto

```
desafio_tecnico_engenharia_dados/
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ docker-compose.yml          # OrquestraÃ§Ã£o de containers
â”‚   â””â”€â”€ Dockerfile                  # Imagem customizada Spark + Python
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ 01_create_schema.sql        # DDL: CriaÃ§Ã£o de tabelas
â”‚   â”œâ”€â”€ 02_insert_data.sql          # DML: Carga de dados fictÃ­cios
â”‚   â”œâ”€â”€ generate_fake_data.py       # Gerador de dados com Faker
â”‚   â””â”€â”€ README.md                   # ğŸ“‹ DocumentaÃ§Ã£o tÃ©cnica detalhada dos scripts SQL
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ etl_pipeline.py             # Pipeline principal ETL
â”‚   â”œâ”€â”€ config.py                   # ConfiguraÃ§Ãµes e variÃ¡veis de ambiente
â”‚   â”œâ”€â”€ data_quality.py             # Sistema de verificaÃ§Ãµes de qualidade
â”‚   â”œâ”€â”€ data_quality.md             # ğŸ“‹ DocumentaÃ§Ã£o tÃ©cnica do sistema de qualidade
â”‚   â””â”€â”€ utils.py                    # FunÃ§Ãµes auxiliares (logging, validaÃ§Ã£o)
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py                 # Fixtures compartilhadas
â”‚   â”œâ”€â”€ test_config.py              # Testes de configuraÃ§Ã£o
â”‚   â”œâ”€â”€ test_utils.py               # Testes de utilitÃ¡rios
â”‚   â”œâ”€â”€ test_etl_pipeline.py        # Testes do pipeline ETL
â”‚   â”œâ”€â”€ test_integration.py         # Testes de integraÃ§Ã£o
â”‚   â”œâ”€â”€ validate_project.py         # Script de validaÃ§Ã£o
â”‚   â””â”€â”€ README.md                   # ğŸ“‹ DocumentaÃ§Ã£o tÃ©cnica detalhada dos testes
â”‚
â”œâ”€â”€ output/                         # DiretÃ³rio para CSV gerado (gitignored)
â”‚
â”œâ”€â”€ .env.example                    # Template de variÃ¡veis de ambiente
â”œâ”€â”€ .gitignore                      # Arquivos ignorados pelo Git
â”œâ”€â”€ requirements.txt                # DependÃªncias Python
â”œâ”€â”€ README.md                       # Este arquivo
â””â”€â”€ desafio.txt                     # DescriÃ§Ã£o original do desafio
```

---

## ğŸ”§ PrÃ©-requisitos

### OpÃ§Ã£o 1: ExecuÃ§Ã£o com Docker (Recomendado)

- [Docker](https://docs.docker.com/get-docker/) >= 20.10
- [Docker Compose](https://docs.docker.com/compose/install/) >= 2.0

### OpÃ§Ã£o 2: ExecuÃ§Ã£o Local

- Python >= 3.10
- MySQL >= 8.0 (jÃ¡ instalado)
- Java JDK >= 11 (requerido pelo Spark)
- Apache Spark >= 3.5

---

## ğŸš€ InstalaÃ§Ã£o e ExecuÃ§Ã£o

### OpÃ§Ã£o 1: Docker (Automatizado)

#### 1. Clone o repositÃ³rio

```bash
git clone https://github.com/ahopache/desafio_tecnico_engenharia_dados.git
cd desafio_tecnico_engenharia_dados
```

#### 2. Configure variÃ¡veis de ambiente

```bash
cp .env.example .env
# Edite .env se necessÃ¡rio (valores padrÃ£o jÃ¡ funcionam)
```

#### 3. Suba o ambiente completo

**MÃ©todo 1: Usando o script run.sh (Recomendado)**
```bash
# Na raiz do projeto
./run.sh
```

**MÃ©todo 2: Comandos manuais**
```bash
# Navegar para o diretÃ³rio docker
cd docker

# Iniciar com arquivo de ambiente especÃ­fico
docker-compose --env-file ../.env up --build -d

# Ou usar variÃ¡veis de ambiente padrÃ£o
docker-compose up --build -d
```

**O que acontece:**
- MySQL Ã© iniciado e populado automaticamente com dados fictÃ­cios
- Container Spark Ã© criado com todas as dependÃªncias
- Rede interna conecta os serviÃ§os
- **Healthchecks garantem** que serviÃ§os estÃ£o prontos antes do uso

#### 4. Execute o pipeline ETL

```bash
# Executa dentro do container Spark
docker-compose exec spark python /app/src/etl_pipeline.py --output /app/output
```

#### 5. Verifique o resultado

```bash
# CSV gerado estarÃ¡ em ./output/movimento_flat.csv
ls -lh ../output/movimento_flat.csv
head ../output/movimento_flat.csv
```

#### 6. Encerre o ambiente

```bash
# Parar todos os serviÃ§os
docker-compose down

# Para remover volumes (dados do MySQL):
docker-compose down -v

# Para parar apenas serviÃ§os especÃ­ficos:
docker-compose stop mysql spark
```

---

### Exemplos PrÃ¡ticos com `--env-file`

#### Exemplo 1: Ambiente de Desenvolvimento
```bash
# Criar arquivo .env especÃ­fico para desenvolvimento
cat > .env.dev << EOF
MYSQL_ROOT_PASSWORD=dev_root_password
MYSQL_DATABASE=dev_sicooperative_db
MYSQL_USER=dev_user
MYSQL_PASSWORD=dev_password
OUTPUT_DIR=./output_dev
LOG_LEVEL=DEBUG
EOF

# Executar com configuraÃ§Ã£o especÃ­fica
docker-compose --env-file .env.dev -f docker/docker-compose.yml up --build -d
```

#### Exemplo 2: Ambiente de ProduÃ§Ã£o
```bash
# Arquivo .env.prod com configuraÃ§Ãµes de produÃ§Ã£o
cat > .env.prod << EOF
MYSQL_ROOT_PASSWORD=${MYSQL_PROD_PASSWORD}
MYSQL_DATABASE=prod_sicooperative_db
MYSQL_USER=${MYSQL_PROD_USER}
MYSQL_PASSWORD=${MYSQL_PROD_PASSWORD}
OUTPUT_DIR=/mnt/data/output
LOG_LEVEL=INFO
OBSERVABILITY_ENABLED=true
EOF

# Executar com configuraÃ§Ã£o de produÃ§Ã£o
docker-compose --env-file .env.prod -f docker/docker-compose.yml up --build -d
```

#### Exemplo 3: ExecuÃ§Ã£o RÃ¡pida
```bash
# Usando o script run.sh com variÃ¡veis customizadas
MYSQL_ROOT_PASSWORD=meu_password ./run.sh
```

### Comandos Ãšteis para Desenvolvimento

```bash
# Ver status dos containers
docker-compose ps

# Ver logs em tempo real
docker-compose logs -f spark

# Executar comandos no container Spark
docker-compose exec spark bash

# Executar apenas os testes
docker-compose exec spark python -m pytest tests/ -v

# Limpar tudo (containers, redes, volumes)
docker-compose down --volumes --remove-orphans

# Reconstruir apenas o container Spark
docker-compose build --no-cache spark
docker-compose up -d spark

---

### OpÃ§Ã£o 2: ExecuÃ§Ã£o Local

#### 1. Configure o banco de dados

**Para usuÃ¡rios avanÃ§ados que desejam configurar o MySQL manualmente:**

```bash
# Scripts SQL detalhados disponÃ­veis em: sql/README.md
mysql -u root -p < sql/01_create_schema.sql
mysql -u root -p < sql/02_insert_data.sql
```

> ğŸ“‹ **Nota**: Para documentaÃ§Ã£o tÃ©cnica completa dos scripts SQL, consulte [`sql/README.md`](sql/README.md)

#### 2. Instale dependÃªncias Python

```bash
pip install -r requirements.txt
```

#### 3. Configure variÃ¡veis de ambiente

```bash
cp .env.example .env
# Edite .env com suas credenciais MySQL
```

#### 4. Execute o pipeline

```bash
python src/etl_pipeline.py --output ./output
```

---

## ğŸ§ª Testes

### Testes UnitÃ¡rios

```bash
# Executar todos os testes
pytest tests/ -v

# Com cobertura
pytest tests/ --cov=src --cov-report=html

# Apenas testes unitÃ¡rios
pytest tests/test_*.py -v
```

**Estrutura dos Testes UnitÃ¡rios:**

```python
tests/
â”œâ”€â”€ test_etl_pipeline.py
â”‚   â”œâ”€â”€ test_read_tables()           # Testa leitura do MySQL
â”‚   â”œâ”€â”€ test_join_tables()           # Valida JOINs
â”‚   â”œâ”€â”€ test_transform_columns()     # Verifica transformaÃ§Ãµes
â”‚   â””â”€â”€ test_write_csv()             # Testa escrita do CSV
â”‚
â””â”€â”€ test_utils.py
    â”œâ”€â”€ test_validate_connection()   # Testa conectividade
    â””â”€â”€ test_logger()                # Valida logging
```

**Cobertura de testes:**
- âœ… **44 testes unitÃ¡rios** distribuÃ­dos em 3 mÃ³dulos
- âœ… **Testes de configuraÃ§Ã£o** (URLs, propriedades, validaÃ§Ãµes)
- âœ… **Testes de utilitÃ¡rios** (logger, validaÃ§Ãµes, formataÃ§Ã£o)
- âœ… **Testes do pipeline** (JOINs, transformaÃ§Ãµes, qualidade)

> ğŸ“‹ **Detalhes tÃ©cnicos**: Para documentaÃ§Ã£o completa dos testes, consulte [`tests/README.md`](tests/README.md)

### Testes de IntegraÃ§Ã£o

**Sistema de Testes End-to-End com Docker Compose:**

```bash
# Teste completo de integraÃ§Ã£o
pytest tests/test_integration.py::TestSiCooperativeIntegration::test_full_pipeline_integration -v -s

# Com diferentes configuraÃ§Ãµes
pytest tests/test_integration.py -k "test_pipeline_with_different_configs" -v

# Ou usar script auxiliar
python run_integration_tests.sh --test-type integration
```

**CenÃ¡rios Testados:**
- âœ… Ambiente Docker sobe corretamente
- âœ… MySQL fica disponÃ­vel e responde
- âœ… Dados de teste sÃ£o inseridos
- âœ… Pipeline ETL executa sem erros
- âœ… Arquivo CSV Ã© gerado corretamente
- âœ… Dados tÃªm estrutura e formato esperados
- âœ… Ambiente Ã© limpo apÃ³s teste

**Dados de Teste:**
- 5 associados com dados realistas
- 5 contas de diferentes tipos
- 5 cartÃµes vinculados
- 25-75 movimentos distribuÃ­dos em 30 dias

**ValidaÃ§Ãµes Realizadas:**
- ExistÃªncia do arquivo `movimento_flat.csv`
- NÃºmero mÃ­nimo de registros (â‰¥ 10)
- Colunas obrigatÃ³rias presentes (11 colunas)
- Tipos de dados corretos
- Variedade de tipos de conta

### Arquivos de Teste

- **`tests/test_integration.py`**: Testes principais de integraÃ§Ã£o
- **`setup_test_data.py`**: Script para popular banco com dados de teste
- **`.env.test`**: ConfiguraÃ§Ãµes especÃ­ficas para ambiente de teste
- **`run_integration_tests.sh`**: Script auxiliar para execuÃ§Ã£o simplificada

### Troubleshooting de Testes

#### Problemas Comuns

1. **Docker nÃ£o disponÃ­vel**
   ```bash
   docker --version && docker-compose --version
   ```

2. **Dados de teste nÃ£o inseridos**
   ```bash
   python setup_test_data.py
   ```

3. **Teste falhando por timeout**
   ```bash
   # Aumentar timeout no teste
   # Modificar _wait_for_mysql(max_attempts=60, delay=5)
   ```

---

## ğŸ”® Melhorias Futuras

## ğŸ“Š Observabilidade e MÃ©tricas

O pipeline implementa um sistema completo de observabilidade para monitoramento em tempo real e anÃ¡lise de performance.

### MÃ©tricas Coletadas

#### MÃ©tricas de Tempo
- **`etl_stage_duration_seconds`**: DuraÃ§Ã£o de cada etapa (extract, transform, load)
- **`etl_pipeline_duration_seconds`**: DuraÃ§Ã£o total do pipeline
- **`etl_quality_check_duration_seconds`**: Tempo gasto em verificaÃ§Ãµes de qualidade

#### MÃ©tricas de Volume
- **`etl_records_input`**: Registros de entrada por etapa
- **`etl_records_output`**: Registros de saÃ­da por etapa
- **`etl_throughput_records_per_second`**: Taxa de processamento

#### MÃ©tricas de Qualidade
- **`etl_quality_checks_total/failed/warnings`**: Status das verificaÃ§Ãµes de qualidade

### ConfiguraÃ§Ã£o

```bash
# Arquivo .env
OBSERVABILITY_ENABLED=true
PROMETHEUS_GATEWAY_URL=http://localhost:9091  # Opcional
PROMETHEUS_JOB_NAME=sicooperative-etl
METRICS_DETAILED_LOGGING=false
METRICS_EXPORT_FILE=pipeline_metrics.json
```

### Sistema de Qualidade de Dados

O pipeline implementa verificaÃ§Ãµes automÃ¡ticas de qualidade durante a execuÃ§Ã£o:

**VerificaÃ§Ãµes implementadas:**
- âœ… **Valores NULL** em campos crÃ­ticos (ex: `id_cartao` â‰¤ 1%)
- âœ… **TransaÃ§Ãµes negativas** em valores financeiros (alerta)
- âœ… **MudanÃ§as drÃ¡sticas** no volume de dados (â‰¤ 50% tolerÃ¢ncia)
- âœ… **Completude** de colunas obrigatÃ³rias

**PolÃ­tica de rejeiÃ§Ã£o:**
- âŒ **FAIL**: Campos crÃ­ticos com NULL excessivo
- âŒ **FAIL**: Colunas obrigatÃ³rias ausentes
- âš ï¸ **WARN**: TransaÃ§Ãµes negativas detectadas
- âš ï¸ **WARN**: MudanÃ§as significativas no volume

> ğŸ“‹ **Detalhes tÃ©cnicos**: Para documentaÃ§Ã£o completa do sistema de qualidade, consulte [`src/data_quality.md`](src/data_quality.md)

### IntegraÃ§Ã£o com Prometheus

O sistema suporta envio automÃ¡tico de mÃ©tricas para o Prometheus via Pushgateway:

```bash
# As mÃ©tricas sÃ£o enviadas automaticamente se PROMETHEUS_GATEWAY_URL estiver configurado
# Verificar mÃ©tricas no Prometheus:
# http://localhost:9090/graph?g0.expr=etl_stage_duration_seconds
```

### Logs Estruturados

```
ğŸ“Š RESUMO DE MÃ‰TRICAS DO PIPELINE
â±ï¸ DuraÃ§Ã£o total: 125.432s
ğŸ“ˆ MÃ©dia por etapa: 17.919s
ğŸ”¢ Total de mÃ©tricas: 45
â³ Timers executados: 7
```

### Dashboards Grafana (Exemplos)

```promql
# Tempo mÃ©dio por etapa
avg(etl_stage_duration_seconds) by (stage)

# Volume de dados processados
sum(etl_records_output) by (stage)

# Qualidade dos dados
etl_quality_checks_failed / etl_quality_checks_total
```

---

## ğŸ§ª Testes de IntegraÃ§Ã£o

### Sistema de Testes End-to-End

O projeto inclui testes de integraÃ§Ã£o completos que validam todo o pipeline usando Docker Compose.

#### CenÃ¡rios Testados

1. **Teste Completo de IntegraÃ§Ã£o**
   - âœ… Ambiente Docker sobe corretamente
   - âœ… MySQL fica disponÃ­vel e responde
   - âœ… Dados de teste sÃ£o inseridos
   - âœ… Pipeline ETL executa sem erros
   - âœ… Arquivo CSV Ã© gerado corretamente
   - âœ… Dados tÃªm estrutura e formato esperados
   - âœ… Ambiente Ã© limpo apÃ³s teste

2. **Teste de SaÃºde do Ambiente**
   - âœ… Containers estÃ£o rodando
   - âœ… MySQL responde a conexÃµes
   - âœ… Rede Docker configurada corretamente

3. **Testes com Diferentes ConfiguraÃ§Ãµes**
   - âœ… ConfiguraÃ§Ã£o padrÃ£o
   - âœ… Com observabilidade habilitada
   - âœ… Com verificaÃ§Ãµes de qualidade

#### Dados de Teste

- **5 associados** com dados realistas
- **5 contas** de diferentes tipos (CORRENTE, POUPANCA, INVESTIMENTO)
- **5 cartÃµes** vinculados Ã s contas e associados
- **25-75 movimentos** distribuÃ­dos nos Ãºltimos 30 dias
- **TransaÃ§Ãµes variadas** (compras, combustÃ­vel, restaurantes, etc.)

#### ExecuÃ§Ã£o dos Testes

```bash
# 1. Subir ambiente de teste
docker-compose -f docker/docker-compose.yml up -d mysql

# 2. Executar setup de dados
python setup_test_data.py

# 3. Executar teste
pytest tests/test_integration.py::TestSiCooperativeIntegration::test_full_pipeline_integration -v -s

# 4. Ou usar script auxiliar
python run_integration_tests.sh --test-type integration
```

#### ValidaÃ§Ãµes Realizadas

- âœ… ExistÃªncia do arquivo `movimento_flat.csv`
- âœ… NÃºmero mÃ­nimo de registros (â‰¥ 10)
- âœ… Colunas obrigatÃ³rias presentes (11 colunas)
- âœ… Tipos de dados corretos
- âœ… Variedade de tipos de conta
- âœ… Valores de transaÃ§Ã£o positivos

### Arquivos de Teste

- **`tests/test_integration.py`**: Testes principais de integraÃ§Ã£o
- **`setup_test_data.py`**: Script para popular banco com dados de teste
- **`.env.test`**: ConfiguraÃ§Ãµes especÃ­ficas para ambiente de teste
- **`run_integration_tests.sh`**: Script auxiliar para execuÃ§Ã£o simplificada

### Troubleshooting de Testes

#### Problemas Comuns

1. **Docker nÃ£o disponÃ­vel**
   ```bash
   # Verificar instalaÃ§Ã£o
   docker --version && docker-compose --version
   ```

2. **Dados de teste nÃ£o inseridos**
   ```bash
   # Executar setup manualmente
   python setup_test_data.py
   ```

3. **Teste falhando por timeout**
   ```bash
   # Aumentar timeout no teste
   # Modificar _wait_for_mysql(max_attempts=60, delay=5)
   ```

### MÃ©tricas de Performance dos Testes

- â±ï¸ **Tempo tÃ­pico**: 2-5 minutos por teste completo
- ğŸ”„ **InicializaÃ§Ã£o**: ~30s (MySQL) + ~60s (dados)
- âš¡ **Pipeline**: ~30-60s dependendo do volume
- ğŸ§¹ **Limpeza**: ~10s
- ğŸ¯ **Taxa de sucesso**: >95% em ambiente limpo

---

## ğŸ”§ Melhorias Futuras

### Com Mais Tempo, Implementaria:

#### **1. PersistÃªncia de Camadas IntermediÃ¡rias**

```python
# Salvar Bronze e Silver em Parquet
df_bronze.write.parquet("s3://datalake/bronze/movimento/")
df_silver.write.parquet("s3://datalake/silver/movimento_joined/")
```

**BenefÃ­cios**:
- Reprocessamento mais rÃ¡pido
- Auditoria de transformaÃ§Ãµes
- RecuperaÃ§Ã£o de falhas

#### **2. Data Quality Checks**

```python
# ValidaÃ§Ãµes entre camadas
- Contagem de registros (sem perda de dados)
- Valores nulos em campos obrigatÃ³rios
- Integridade referencial
- Ranges de valores (ex: idade > 0)
```

**Framework**: Great Expectations ou Deequ

#### **3. Particionamento Inteligente**

```python
# Particionar por data para queries eficientes
df.write.partitionBy("ano", "mes").parquet("output/")
```

**BenefÃ­cios**:
- Queries mais rÃ¡pidas (partition pruning)
- Processamento incremental
- Menor custo de storage

#### **4. OrquestraÃ§Ã£o com Airflow**

```python
# DAG para execuÃ§Ã£o agendada
dag = DAG('sicooperative_etl', schedule_interval='@daily')

tasks:
  extract â†’ transform â†’ validate â†’ load â†’ notify
```

**BenefÃ­cios**:
- Agendamento automÃ¡tico
- Retry em falhas
- Monitoramento visual
- Alertas

#### **5. Monitoramento e Observabilidade AvanÃ§ada**

**Sistema jÃ¡ implementado** (detalhado na seÃ§Ã£o [ Observabilidade e MÃ©tricas](#-observabilidade-e-mÃ©tricas)):

```python
# MÃ©tricas jÃ¡ coletadas automaticamente:
- etl_stage_duration_seconds (por etapa)
- etl_records_output (volume de dados)
- etl_quality_checks_failed (qualidade)
- etl_pipeline_duration_seconds (total)
```

**IntegraÃ§Ãµes futuras**:
- **Alertas automÃ¡ticos**: Slack/Email em falhas crÃ­ticas
- **Dashboards executivos**: RelatÃ³rios automatizados
- **SLA monitoring**: Controle de acordos de nÃ­vel de serviÃ§o
- **AnÃ¡lise de tendÃªncias**: DetecÃ§Ã£o proativa de problemas

#### **7. CI/CD Pipeline**

```yaml
# GitHub Actions
- Lint (flake8, black)
- Testes unitÃ¡rios
- Testes de integraÃ§Ã£o
- Build de imagem Docker
- Deploy automatizado
```

#### **8. SeguranÃ§a**

- **Secrets Management**: AWS Secrets Manager ou Vault
- **Criptografia**: Dados em trÃ¢nsito (TLS) e em repouso
- **IAM**: Controle de acesso granular
- **Audit Logs**: Rastreabilidade de acessos

#### **9. Performance**

- **Caching**: Spark cache para tabelas reutilizadas
- **Broadcast Joins**: ForÃ§ar broadcast para tabelas pequenas
- **Bucketing**: PrÃ©-particionar por chave de JOIN
- **Z-Ordering**: Otimizar layout de arquivos Parquet

---

## ğŸ› Dificuldades Encontradas

### 1. **Compatibilidade de VersÃµes**

**Problema**: Conflito entre versÃµes do MySQL Connector e PySpark

**SoluÃ§Ã£o**:
- Fixei versÃµes especÃ­ficas no `requirements.txt`
- Testei compatibilidade: PySpark 3.5 + mysql-connector-java 8.0.33
- Documentei versÃµes testadas

### 2. **ConfiguraÃ§Ã£o JDBC no Spark**

**Problema**: Erro de ClassNotFoundException para driver MySQL

**SoluÃ§Ã£o**:
```python
# Adicionar JAR do MySQL ao classpath do Spark
spark = SparkSession.builder \
    .config("spark.jars", "/path/to/mysql-connector.jar") \
    .getOrCreate()
```

### 3. **Networking Docker**

**Problema**: Container Spark nÃ£o conseguia conectar ao MySQL

**SoluÃ§Ã£o**:
- Configurei rede customizada no Docker Compose
- Usei nome do serviÃ§o como hostname (`mysql:3306`)
- Aguardei MySQL estar "healthy" antes de iniciar Spark

### 4. **Encoding de Caracteres**

**Problema**: Caracteres especiais (acentos) corrompidos no CSV

**SoluÃ§Ã£o**:
```python
df.write.option("encoding", "UTF-8").csv(output_path)
```

### 5. **GeraÃ§Ã£o de Dados FictÃ­cios Consistentes**

**Problema**: Garantir integridade referencial entre tabelas

**SoluÃ§Ã£o**:
- Usei Faker com seed fixo para reprodutibilidade
- Gerei IDs sequenciais
- Validei foreign keys antes de inserir

---

## ğŸ“Š Modelo de Dados

### Diagrama ER

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Associado     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id (PK)         â”‚
â”‚ nome            â”‚
â”‚ sobrenome       â”‚
â”‚ idade           â”‚
â”‚ email           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 1
         â”‚
         â”‚ N
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Conta       â”‚       â”‚     CartÃ£o      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id (PK)         â”‚â—„â”€â”€â”€â”€â”€â”€â”¤ id (PK)         â”‚
â”‚ tipo            â”‚ 1   N â”‚ num_cartao      â”‚
â”‚ data_criacao    â”‚       â”‚ nom_impresso    â”‚
â”‚ id_associado(FK)â”‚       â”‚ id_conta (FK)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚ id_associado(FK)â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚ 1
                                   â”‚
                                   â”‚ N
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚   Movimento     â”‚
                          â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                          â”‚ id (PK)         â”‚
                          â”‚ vlr_transacao   â”‚
                          â”‚ des_transacao   â”‚
                          â”‚ data_movimento  â”‚
                          â”‚ id_cartao (FK)  â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Schema do CSV Final

| Coluna | Tipo | Origem | DescriÃ§Ã£o |
|--------|------|--------|-----------|
| `nome_associado` | string | associado.nome | Nome do cliente |
| `sobrenome_associado` | string | associado.sobrenome | Sobrenome do cliente |
| `idade_associado` | string | associado.idade | Idade do cliente |
| `vlr_transacao_movimento` | string | movimento.vlr_transacao | Valor da transaÃ§Ã£o |
| `des_transacao_movimento` | string | movimento.des_transacao | DescriÃ§Ã£o da transaÃ§Ã£o |
| `data_movimento` | string | movimento.data_movimento | Data/hora da transaÃ§Ã£o |
| `numero_cartao` | string | cartao.num_cartao | NÃºmero do cartÃ£o |
| `nome_impresso_cartao` | string | cartao.nom_impresso | Nome impresso no cartÃ£o |
| `data_criacao_cartao` | string | cartao.data_criacao | Data de criaÃ§Ã£o do cartÃ£o |
| `tipo_conta` | string | conta.tipo | Tipo da conta (corrente/poupanÃ§a) |
| `data_criacao_conta` | string | conta.data_criacao | Data de abertura da conta |

---

## âœ¨ Formatos de SaÃ­da
O pipeline suporta geraÃ§Ã£o de dados em mÃºltiplos formatos, configurÃ¡veis via variÃ¡vel de ambiente `OUTPUT_FORMAT`:
### 1. CSV (PadrÃ£o)
- **Vantagens**:
 - CompatÃ­vel com qualquer ferramenta
 - FÃ¡cil inspeÃ§Ã£o manual
 - Ãšnico arquivo de saÃ­da
- **Uso recomendado**:
 - Compartilhamento de dados
 - Ferramentas que nÃ£o suportam Parquet
### 2. Parquet (Recomendado para anÃ¡lise)
- **Vantagens**:
 - Formato colunar otimizado para consultas analÃ­ticas
 - Particionamento por data (`data_movimento_date`)
 - CompressÃ£o eficiente (Snappy por padrÃ£o)
 - PreservaÃ§Ã£o de tipos de dados
- **Uso recomendado**:
 - Processamento com Spark/PySpark
 - AnÃ¡lises em grandes volumes de dados
 - Ambientes de Data Lake
### ConfiguraÃ§Ã£o
No arquivo `.env`:
```ini
# Formatos de saÃ­da (csv, parquet ou ambos)
OUTPUT_FORMAT=csv,parquet
# ConfiguraÃ§Ãµes do Parquet
PARQUET_COMPRESSION=snappy # OpÃ§Ãµes: none, snappy, gzip, lzo, brotli, lz4
```

## ğŸ“Š Exemplo de SaÃ­da

### Dados de Exemplo
```sql
-- Antes
numero_cartao: 1234567890123456
email: joao.silva@email.com

-- Depois
numero_cartao_masked: 123456******3456
numero_cartao_hash: a1b2c3... (hash SHA-256)
email_masked: joa****@email.com
email_hash: x9y8z7... (hash SHA-256)
```

### Como Usar os Dados Gerados

#### 1. Lendo o CSV
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("ReadCSV").getOrCreate()

# Ler o arquivo CSV
df_csv = spark.read \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .csv("output/csv/")

df_csv.show(5)
```

#### 2. Lendo o Parquet (Recomendado)
```python
# Ler Parquet particionado
df_parquet = spark.read.parquet("output/parquet/")

# Consulta otimizada com filtro de partiÃ§Ã£o
df_jan_2023 = df_parquet.filter("data_movimento_date = '2023-01-01'")

# AnÃ¡lise de exemplo: total por categoria
df_parquet.groupBy("categoria") \
    .agg({"vlr_transacao_movimento": "sum"}) \
    .show()
```

#### 3. Convertendo para Pandas (para anÃ¡lise em memÃ³ria)
```python
# Converter para Pandas (apenas se os dados couberem em memÃ³ria)
import pandas as pd

# Para CSV
pdf_csv = pd.read_csv("output/csv/part-00000-*.csv")

# Para Parquet (usando pyarrow para melhor performance)
pdf_parquet = pd.read_parquet("output/parquet/")
```

#### 4. Consultas SQL
```python
# Registrar o DataFrame como uma view temporÃ¡ria
df_parquet.createOrReplaceTempView("movimentos")

# Executar consulta SQL
result = spark.sql("""
    SELECT 
        nome_associado,
        COUNT(*) as total_compras,
        SUM(CAST(vlr_transacao_movimento AS DECIMAL(10,2))) as valor_total
    FROM movimentos
    WHERE data_movimento_date BETWEEN '2023-01-01' AND '2023-01-31'
    GROUP BY nome_associado
    ORDER BY valor_total DESC
""")

result.show()
```

---

## ğŸ“‹ RelatÃ³rio de ValidaÃ§Ã£o

Para detalhes tÃ©cnicos sobre validaÃ§Ã£o do projeto, mÃ©tricas de qualidade e anÃ¡lise de implementaÃ§Ã£o, consulte o arquivo [`report/VALIDATION_REPORT.md`](report/VALIDATION_REPORT.md).

Este relatÃ³rio inclui:
- âœ… **ValidaÃ§Ãµes tÃ©cnicas** realizadas
- âœ… **EstatÃ­sticas do projeto** (linhas de cÃ³digo, testes, cobertura)
- âœ… **VerificaÃ§Ã£o de requisitos** do desafio
- âœ… **Diferenciais implementados** em tabela detalhada
- âœ… **PrÃ³ximos passos** para execuÃ§Ã£o e deploy

---

## ğŸ“ LicenÃ§a

Este projeto foi desenvolvido como parte de um desafio tÃ©cnico para vaga de Engenheiro de Dados SÃªnior.

---

## ğŸ‘¤ Autor

Assis Henrique Oliveira Pacheco
- GitHub: [@ahopache](https://github.com/ahopache)
- LinkedIn: [Assis Henrique](https://www.linkedin.com/in/assispacheco/)

---

## ğŸ™ Agradecimentos

Obrigado pela oportunidade de participar deste desafio tÃ©cnico! Foi uma experiÃªncia enriquecedora aplicar conceitos de arquitetura de Data Lake e engenharia de dados em um cenÃ¡rio realista.

---

**Desenvolvido com muita atenÃ§Ã£o e cuidado para o desafio tÃ©cnico de Engenharia de Dados**